{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process test data\n",
    "\n",
    "This notebook takes you through the steps of how to preprocess a high S/N and low S/N test set\n",
    "* required packages: numpy, h5py, vos\n",
    "* required data files: apStar_combined_main.h5 and training_data.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset keys in file: \n",
      "\n",
      "['0_FE', '0_FE_ERR', 'ALPHA_M', 'AL_FE', 'AL_FE_ERR', 'CA_FE', 'CA_FE_ERR', 'C_FE', 'C_FE_ERR', 'FE_H', 'FE_H_ERR', 'IDs', 'K_FE', 'K_FE_ERR', 'LOGG', 'LOGG_ERR', 'MG_FE', 'MG_FE_ERR', 'MN_FE', 'MN_FE_ERR', 'Main Sequence Flag', 'NA_FE', 'NA_FE_ERR', 'NI_FE', 'NI_FE_ERR', 'N_FE', 'N_FE_ERR', 'PARAM', 'SI_FE', 'SI_FE_ERR', 'S_FE', 'S_FE_ERR', 'TEFF', 'TEFF_ERR', 'TI_FE', 'TI_FE_ERR', 'VRAD', 'VRAD_ERR', 'VSCATTER', 'V_FE', 'V_FE_ERR', 'aspcap_flag', 'error_spectrum', 'num_visits', 'spectrum', 'stacked_snr', 'star_flag', 'targ1_flag', 'targ2_flag']\n",
      "['TEFF', 'FE_H', 'ALPHA_M', 'C_FE', 'N_FE', 'stacked_snr', 'LOGG', 'star_flag', 'aspcap_flag', 'VSCATTER']\n",
      "Obtained data for 143482 stars.\n",
      "main flags [35591]\n",
      "FE_H flags [35591]\n",
      "ALPHA_M flags [35591]\n",
      "C_FE flags [35588]\n",
      "N_FE flags [35587]\n",
      "['TEFF', 'FE_H', 'ALPHA_M', 'C_FE', 'N_FE', 'stacked_snr', 'LOGG', 'star_flag', 'aspcap_flag', 'VSCATTER']\n",
      "35507 stars remain.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "import GetData\n",
    "\n",
    "datadir = GetData.getDataDir()\n",
    "F = h5py.File(datadir + 'apStar_combined_main.h5','r')\n",
    "data = GetData.get(F, False)\n",
    "\n",
    "indices = data['indices']\n",
    "data['IDs'] = data['IDs'][indices]\n",
    "for col in GetData.getCols():\n",
    "    data[col] = data[col][indices]\n",
    "\n",
    "print(str(len(list(set(list(data['IDs'])))))+' stars remain.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collect label normalization data**\n",
    "\n",
    "Create a file that contains the mean and standard deviation for parameters in order to normalize labels during training and testing. Note, this is done after we have eliminated those records we don't want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_and_std.npy saved\n"
     ]
    }
   ],
   "source": [
    "params = GetData.getParams()\n",
    "mean = [np.mean(data[p]) for p in params]\n",
    "std = [np.std(data[p]) for p in params]\n",
    "mean_and_std = np.row_stack((np.array(mean),np.array(std)))\n",
    "np.save(datadir+'mean_and_std', mean_and_std)\n",
    "\n",
    "print('mean_and_std.npy saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load test set APOGEE IDs**\n",
    "\n",
    "Load previously created file that contains the training data. We do not want to include any of the APOGEE IDs used in the training set in our test set. This file was created in 2_Preprocessing_of_Training_Data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "savename = 'training_data.h5'\n",
    "\n",
    "with h5py.File(datadir + savename, \"r\") as f:\n",
    "    train_ap_id = f['Ap_ID'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separate data for High S/N test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set includes 21029 combined spectra\n"
     ]
    }
   ],
   "source": [
    "indices_test = [i for i, item in enumerate(data['IDs']) if item not in train_ap_id]\n",
    "test_ap_id = data['IDs'][indices_test]\n",
    "test_combined_snr = data['stacked_snr'][indices_test]\n",
    "test_data = {}\n",
    "for p in GetData.getParams():\n",
    "    test_data[p] = data[p][indices_test]\n",
    "\n",
    "indices_test_set = indices[indices_test] # These indices will be used to index through the spectra\n",
    "\n",
    "print('Test set includes '+str(len(test_ap_id))+' combined spectra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now collect spectra and error spectra. Then normalize each spectrum and save the data**\n",
    "\n",
    "**Steps taken to normalize spectra:**\n",
    "1. separate into three chips\n",
    "2. divide by median value in each chip\n",
    "3. recombine each spectrum into a vector of 7214 flux values\n",
    "4. Error spectra must also be normalized with the same median values for use in the error propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define edges of detectors\n",
    "blue_chip_begin = 322\n",
    "blue_chip_end = 3242\n",
    "green_chip_begin = 3648\n",
    "green_chip_end = 6048   \n",
    "red_chip_begin = 6412\n",
    "red_chip_end = 8306 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data.h5 has been saved as the test set to be used in 5_Test_Model.ipynb\n"
     ]
    }
   ],
   "source": [
    "savename = 'test_data.h5'\n",
    "\n",
    "with h5py.File(datadir + savename, \"w\") as f:\n",
    "    \n",
    "    # Create datasets for your test data file \n",
    "    spectra_ds = f.create_dataset('spectrum', (1,7214), maxshape=(None,7214), dtype=\"f\", chunks=(1,7214))\n",
    "    error_spectra_ds = f.create_dataset('error_spectrum', (1,7214), maxshape=(None,7214), dtype=\"f\", chunks=(1,7214))\n",
    "    ap_id_ds = f.create_dataset('Ap_ID', test_ap_id.shape, dtype=\"S18\")\n",
    "    ap_id_ds[:] = test_ap_id.tolist()\n",
    "    combined_snr_ds = f.create_dataset('combined_snr', test_combined_snr.shape, dtype=\"f\")\n",
    "    combined_snr_ds[:] = test_combined_snr\n",
    "    for p in GetData.getParams():\n",
    "        p_ds = f.create_dataset(p, test_data[p].shape, dtype=\"f\")\n",
    "        # Save data to data file\n",
    "        p_ds[:] = test_data[p]\n",
    "    \n",
    "    # Collect spectra\n",
    "    first_entry=True\n",
    "    \n",
    "    for i in indices_test_set:\n",
    "\n",
    "        spectrum = F['spectrum'][i:i+1]\n",
    "        spectrum[np.isnan(spectrum)]=0.\n",
    "        \n",
    "        err_spectrum = F['error_spectrum'][i:i+1]\n",
    "\n",
    "        # NORMALIZE SPECTRUM\n",
    "        # Separate spectra into chips\n",
    "        blue_sp = spectrum[0:1,blue_chip_begin:blue_chip_end]\n",
    "        green_sp = spectrum[0:1,green_chip_begin:green_chip_end]\n",
    "        red_sp = spectrum[0:1,red_chip_begin:red_chip_end]\n",
    "        \n",
    "        blue_sp_med = np.median(blue_sp, axis=1)\n",
    "        green_sp_med = np.median(green_sp, axis=1)\n",
    "        red_sp_med = np.median(red_sp, axis=1)\n",
    "\n",
    "        # Normalize spectra by chips\n",
    "        blue_sp = (blue_sp.T/blue_sp_med).T\n",
    "        green_sp = (green_sp.T/green_sp_med).T\n",
    "        red_sp = (red_sp.T/red_sp_med).T\n",
    "\n",
    "        # Recombine spectra\n",
    "        spectrum = np.column_stack((blue_sp,green_sp,red_sp))\n",
    "        \n",
    "        # Normalize error spectrum using the same method\n",
    "        # Separate error spectra into chips\n",
    "\n",
    "        blue_sp = err_spectrum[0:1,blue_chip_begin:blue_chip_end]\n",
    "        green_sp = err_spectrum[0:1,green_chip_begin:green_chip_end]\n",
    "        red_sp = err_spectrum[0:1,red_chip_begin:red_chip_end]\n",
    "\n",
    "        # Normalize error spectra by chips\n",
    "        blue_sp = (blue_sp.T/blue_sp_med).T\n",
    "        green_sp = (green_sp.T/green_sp_med).T\n",
    "        red_sp = (red_sp.T/red_sp_med).T\n",
    "\n",
    "        # Recombine error spectra\n",
    "        err_spectrum = np.column_stack((blue_sp,green_sp,red_sp))\n",
    "        \n",
    "        \n",
    "        if first_entry:\n",
    "            spectra_ds[0] = spectrum\n",
    "            error_spectra_ds[0] = err_spectrum\n",
    "            \n",
    "            first_entry=False\n",
    "        else:\n",
    "            spectra_ds.resize(spectra_ds.shape[0]+1, axis=0)\n",
    "            error_spectra_ds.resize(error_spectra_ds.shape[0]+1, axis=0)\n",
    "\n",
    "            spectra_ds[-1] = spectrum\n",
    "            error_spectra_ds[-1] = err_spectrum\n",
    "\n",
    "print(savename+' has been saved as the test set to be used in 5_Test_Model.ipynb')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
