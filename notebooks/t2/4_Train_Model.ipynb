{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the StarNet Model\n",
    "\n",
    "This notebook takes you through the steps of how to train a StarNet Model\n",
    "- Required Python packages: `numpy h5py keras`\n",
    "- Required data files: training_data.h5, mean_and_std.npy\n",
    "\n",
    "Note: We use tensorflow for the keras backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer, Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "datadir = \"/media/apogee/starnet/t2/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Batch the Training Data **\n",
    "\n",
    "Define a function that will return a batch of data from an h5 file in the form of an HDF5 matrix. This enables the user to only load a batch of spectra at a time while training.\n",
    "\n",
    "Within this function the labels will be normalized to approximately have a mean of zero and unit variance.\n",
    "\n",
    "NOTE: This is necessary to put output labels on a similar scale in order for the model to train properly, this process is reversed in the test stage to give the output labels their proper units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch_from_h5(data_file, num_objects, batch_size, indx, mu_std=''):\n",
    "        \n",
    "        mean_and_std = np.load(mu_std)\n",
    "        mean_labels = mean_and_std[0]\n",
    "        std_labels = mean_and_std[1]\n",
    "            \n",
    "        # Generate list of random indices (within the relevant partition of the main data file, e.g. the\n",
    "        # training set) to be used to index into data_file\n",
    "        indices = random.sample(range(indx, indx+num_objects), batch_size)\n",
    "        indices = np.sort(indices)\n",
    "        \n",
    "        # load data\n",
    "        F = h5py.File(data_file, 'r')\n",
    "        X = F['spectrum']\n",
    "        # -PS #teff = F['TEFF']\n",
    "        # -PS #logg = F['LOGG']\n",
    "        fe_h = F['FE_H']\n",
    "        c_fe = F['C_FE'] # +PS\n",
    "        n_fe = F['N_FE'] # +PS\n",
    "        \n",
    "        X = X[indices,:]\n",
    "        #mask any nan values\n",
    "        indices_nan = np.where(np.isnan(X))\n",
    "        X[indices_nan]=0.\n",
    "        \n",
    "        y = np.column_stack((fe_h[:][indices],\n",
    "                                 # -PS #teff[:][indices],\n",
    "                                 # -PS #logg[:][indices],\n",
    "                                 c_fe[:][indices], # +PS\n",
    "                                 n_fe[:][indices])) # +PS\n",
    "        \n",
    "        # Normalize labels\n",
    "        normed_y = (y-mean_labels) / std_labels\n",
    "        \n",
    "        # Reshape X data for compatibility with CNN\n",
    "        X = X.reshape(len(X), 7214, 1)\n",
    "        \n",
    "        return X, normed_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create Batch Generators for the Training and Cross-Validation Data **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_batch(data_file, num_objects, batch_size, indx, mu_std):\n",
    "    \n",
    "    while True:\n",
    "        x_batch, y_batch = load_batch_from_h5(data_file, \n",
    "                                                   num_objects, \n",
    "                                                   batch_size, \n",
    "                                                   indx, \n",
    "                                                   mu_std)\n",
    "        yield (x_batch, y_batch)\n",
    "\n",
    "def generate_cv_batch(data_file, num_objects, batch_size, indx, mu_std):\n",
    "    \n",
    "    while True:\n",
    "        x_batch, y_batch = load_batch_from_h5(data_file, \n",
    "                                                   num_objects, \n",
    "                                                   batch_size, \n",
    "                                                   indx, \n",
    "                                                   mu_std)\n",
    "        yield (x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Obtain information from the reference set and the normalization data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = 'training_data.h5'\n",
    "normalization_data = 'mean_and_std.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each spectrum contains 7214 wavelength bins\n",
      "Training set includes 41000 spectra and the cross-validation set includes 3784 spectra\n"
     ]
    }
   ],
   "source": [
    "# Define the number of output labels\n",
    "num_labels = np.load(datadir+'mean_and_std.npy').shape[1]\n",
    "\n",
    "# Define the number of training spectra\n",
    "num_train = 41000\n",
    "\n",
    "with h5py.File(datadir+training_set, 'r') as F:\n",
    "    spectra = F['spectrum']\n",
    "    num_flux = spectra.shape[1]\n",
    "    num_cv = spectra.shape[0]-num_train\n",
    "print('Each spectrum contains ' + str(num_flux) + ' wavelength bins')\n",
    "print('Training set includes ' + str(num_train) + ' spectra and the cross-validation set includes ' + str(num_cv)+' spectra')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build the StarNet model architecture**\n",
    "\n",
    "The StarNet architecture is built with:\n",
    "- input layer\n",
    "- 2 convolutional layers\n",
    "- 1 maxpooling layer followed by flattening for the fully connected layer\n",
    "- 2 fully connected layers\n",
    "- output layer\n",
    "\n",
    "First, let's define some model variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function used following every layer except for the output layers\n",
    "activation = 'relu'\n",
    "\n",
    "# model weight initializer\n",
    "initializer = 'he_normal'\n",
    "\n",
    "# shape of input spectra that is fed into the input layer\n",
    "input_shape = (None, num_flux, 1)\n",
    "\n",
    "# number of filters used in the convolutional layers\n",
    "num_filters = [4,16]\n",
    "\n",
    "# length of the filters in the convolutional layers\n",
    "filter_length = 8\n",
    "\n",
    "# length of the maxpooling window \n",
    "pool_length = 4\n",
    "\n",
    "# number of nodes in each of the hidden fully connected layers\n",
    "num_hidden = [256,128]\n",
    "\n",
    "# number of spectra fed into model at once during training\n",
    "batch_size = 64\n",
    "\n",
    "# maximum number of interations for model training\n",
    "max_epochs = 30\n",
    "\n",
    "# initial learning rate for optimization algorithm\n",
    "lr = 0.0007\n",
    "    \n",
    "# exponential decay rate for the 1st moment estimates for optimization algorithm\n",
    "beta_1 = 0.9\n",
    "\n",
    "# exponential decay rate for the 2nd moment estimates for optimization algorithm\n",
    "beta_2 = 0.999\n",
    "\n",
    "# a small constant for numerical stability for optimization algorithm\n",
    "optimizer_epsilon = 1e-08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    InputLayer(batch_input_shape=input_shape),\n",
    "    Conv1D(kernel_initializer=initializer, activation=activation, padding=\"same\", filters=num_filters[0], kernel_size=filter_length),\n",
    "    Conv1D(kernel_initializer=initializer, activation=activation, padding=\"same\", filters=num_filters[1], kernel_size=filter_length),\n",
    "    MaxPooling1D(pool_size=pool_length),\n",
    "    Flatten(),\n",
    "    Dense(units=num_hidden[0], kernel_initializer=initializer, activation=activation),\n",
    "    Dense(units=num_hidden[1], kernel_initializer=initializer, activation=activation),\n",
    "    Dense(units=num_labels, activation=\"linear\", input_dim=num_hidden[1]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More model techniques**\n",
    "* The `Adam` optimizer is the gradient descent algorithm used for minimizing the loss function\n",
    "* `EarlyStopping` uses the cross-validation set to test the model following every iteration and stops the training if the cv loss does not decrease by `min_delta` after `patience` iterations\n",
    "* `ReduceLROnPlateau` is a form of learning rate decay where the learning rate is decreased by a factor of `factor` if the training loss does not decrease by `epsilon` after `patience` iterations unless the learning rate has reached `min_lr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default loss function parameters\n",
    "early_stopping_min_delta = 0.0001\n",
    "early_stopping_patience = 4\n",
    "reduce_lr_factor = 0.5\n",
    "reuce_lr_epsilon = 0.0009\n",
    "reduce_lr_patience = 2\n",
    "reduce_lr_min = 0.00008\n",
    "\n",
    "# loss function to minimize\n",
    "loss_function = 'mean_squared_error'\n",
    "\n",
    "# compute accuracy and mean absolute deviation\n",
    "metrics = ['accuracy', 'mae']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/keras/callbacks.py:928: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` insted.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(lr=lr, beta_1=beta_1, beta_2=beta_2, epsilon=optimizer_epsilon, decay=0.0)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=early_stopping_min_delta, \n",
    "                                       patience=early_stopping_patience, verbose=2, mode='min')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, epsilon=reuce_lr_epsilon, \n",
    "                                  patience=reduce_lr_patience, min_lr=reduce_lr_min, mode='min', verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., steps_per_epoch=640.625, epochs=30, validation_data=<generator..., verbose=2, callbacks=[<keras.ca..., validation_steps=59.125, max_queue_size=10)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 66s - loss: 13311.1083 - acc: 0.6856 - mean_absolute_error: 1.0266 - val_loss: 71083.2828 - val_acc: 0.7521 - val_mean_absolute_error: 2.6462\n",
      "Epoch 2/30\n",
      " - 65s - loss: 33431.5043 - acc: 0.6778 - mean_absolute_error: 2.0940 - val_loss: 107465.1112 - val_acc: 0.7372 - val_mean_absolute_error: 4.4948\n",
      "Epoch 3/30\n",
      " - 65s - loss: 31645.5581 - acc: 0.6919 - mean_absolute_error: 2.2762 - val_loss: 107467.2644 - val_acc: 0.7555 - val_mean_absolute_error: 4.5368\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n",
      "Epoch 4/30\n",
      " - 66s - loss: 28391.1939 - acc: 0.5806 - mean_absolute_error: 2.2346 - val_loss: 213095.3607 - val_acc: 0.2555 - val_mean_absolute_error: 9.2894\n",
      "Epoch 5/30\n",
      " - 66s - loss: 11676.3027 - acc: 0.6698 - mean_absolute_error: 1.2180 - val_loss: 178413.9414 - val_acc: 0.7661 - val_mean_absolute_error: 7.1609\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f06374a8630>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generate_train_batch(datadir+training_set, \n",
    "                                         num_train, batch_size, 0, \n",
    "                                         datadir+normalization_data),\n",
    "                                  steps_per_epoch = num_train/batch_size,\n",
    "                                  epochs=max_epochs,\n",
    "                                  validation_data=generate_cv_batch(datadir+training_set, \n",
    "                                                                    num_cv, batch_size, num_train,\n",
    "                                                                    datadir+normalization_data),\n",
    "                                  max_q_size=10, verbose=2,\n",
    "                                  callbacks=[early_stopping, reduce_lr],\n",
    "                                 validation_steps=num_cv/batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starnet_cnn.h5 saved.\n"
     ]
    }
   ],
   "source": [
    "starnet_model = 'starnet_cnn.h5'\n",
    "model.save(datadir + starnet_model)\n",
    "print(starnet_model+' saved.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
